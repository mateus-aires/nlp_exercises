{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MEg1z9k4wuO_"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contagem de palavras e análise de livros\n",
    "###### Aluno: Mateus Cavalcante de Almeida Farias Aires\n",
    "\n",
    "Os livros escolhidos foram: \n",
    "\"On the Origin of Species\", de Charles Darwin;\n",
    "\"The Brothers Karamazov\", de Fyodor Dostoyevsky; e\n",
    "\"The Trial\", de Franz Kafka;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "id": "htfenOm5zfvD",
    "outputId": "dc548c19-82ed-4cae-f840-c526d5110468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\" xml:lang=\"en\">\n",
      "<head>\n",
      "<meta charset=\"utf-8\">\n",
      "<style>.xhtml_big {font-size: larger;}</style>\n",
      "<meta name=\"generator\" content=\"HTML Tidy for HTML5 for Linux version 5.6.0\">\n",
      "<title>The Project Gutenberg eBook of The Brothers Karamazov by Fyodor Dostoyevsky</title>\n",
      "\n",
      "<style>/**/\n",
      "body {\n",
      "    margin-left: 20%;\n",
      "    margin-right: 20%;\n",
      "    text-align: justify\n",
      "    }\n",
      "h1, h2, h3, h4, h5 {\n",
      "    text-align: center;\n",
      "    font-style: normal;\n",
      "    font-weight: normal;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os_request = requests.get(\"https://www.gutenberg.org/cache/epub/22764/pg22764-images.html\") \n",
    "fam_request = requests.get(\"https://www.gutenberg.org/cache/epub/28054/pg28054-images.html\")\n",
    "trial_request = requests.get(\"https://www.gutenberg.org/cache/epub/7849/pg7849.html\")\n",
    "\n",
    "os_request.encoding = 'utf-8'\n",
    "fam_request.encoding = 'utf-8'\n",
    "trial_request.encoding = 'utf-8'\n",
    "\n",
    "html1 = os_request.text\n",
    "html2 = fam_request.text\n",
    "html3 = trial_request.text\n",
    "\n",
    "print(html2[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 919
    },
    "id": "Z4lC4y_Izzce",
    "outputId": "85726531-aeaf-4894-faf8-f4656004f0e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ividuals of different species, is sometimes inherited and sometimes not so; why the child often reve\n",
      "’s face at that minute. He used to say that it was frenzied but beautiful as he remembered. But he r\n",
      "emen leaving - the\n",
      "supervisor had stopped him noticing the three bank staff and now the\n",
      "three bank\n"
     ]
    }
   ],
   "source": [
    "soup1 = BeautifulSoup(html1, 'html.parser')\n",
    "soup2 = BeautifulSoup(html2, 'html.parser')\n",
    "soup3 = BeautifulSoup(html3, 'html.parser')\n",
    "\n",
    "os_text = soup1.get_text()[783:-18724]\n",
    "fam_text = soup2.get_text()[4871:-19054]\n",
    "trial_text = soup3.get_text()[1495:-18433]\n",
    "\n",
    "print(os_text[32000:32100])\n",
    "print(fam_text[32000:32100])\n",
    "print(trial_text[32000:32100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "zC1lUqpn0MwJ",
    "outputId": "9e9844cb-e3d5-43fd-8829-25748ef4e709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mitya', 'a', 'child', 'of', 'three', 'years', 'old', 'in', 'her', 'husband']\n",
      "86332\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "\n",
    "os_tokens = tokenizer.tokenize(os_text)\n",
    "fam_tokens = tokenizer.tokenize(fam_text)\n",
    "trial_tokens = tokenizer.tokenize(trial_text)\n",
    "\n",
    "print(fam_tokens[1000:1010])\n",
    "print(len(trial_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mateus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "JV2rJSzH4Wdr",
    "outputId": "5fc23671-2285-4462-fbee-02286f89a1d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year', 'sent', 'memoir', 'subject', 'request', 'would', 'forward', 'sir', 'charles', 'lyell']\n",
      "36749\n"
     ]
    }
   ],
   "source": [
    "# Vamos filtrar as stop-words, e converter todas as palavras para letras minúsculas\n",
    "os_words = [word.lower() for word in os_tokens if word.lower() not in sw]\n",
    "fam_words = [word.lower() for word in fam_tokens if word.lower() not in sw]\n",
    "trial_words = [word.lower() for word in trial_tokens if word.lower() not in sw]\n",
    "\n",
    "print(os_words[1000:1010])\n",
    "print(len(trial_words)) # Quase metade dos tokens foram filtrados por serem stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Contagem de Palavras no Moby Dick.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
